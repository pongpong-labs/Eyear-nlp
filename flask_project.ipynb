{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [21/Aug/2020 02:37:37] \"\u001b[37mPOST /correctSubtitle HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2020 02:37:49] \"\u001b[37mPOST /correctSubtitle HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2020 02:37:59] \"\u001b[37mPOST /correctSubtitle HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2020 02:38:23] \"\u001b[37mPOST /correctSubtitle HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2020 02:38:39] \"\u001b[37mPOST /correctSubtitle HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2020 02:38:54] \"\u001b[37mPOST /correctSubtitle HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2020 02:42:13] \"\u001b[37mPOST /correctSubtitle HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2020 02:42:22] \"\u001b[37mPOST /correctSubtitle HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2020 02:42:26] \"\u001b[37mPOST /correctSubtitle HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2020 02:42:31] \"\u001b[37mPOST /correctSubtitle HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2020 02:43:35] \"\u001b[37mPOST /correctSubtitle HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2020 02:43:36] \"\u001b[37mPOST /correctSubtitle HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2020 02:43:37] \"\u001b[37mPOST /correctSubtitle HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from eunjeon import Mecab\n",
    "from korean_romanizer import *\n",
    "import jellyfish\n",
    "from pykospacing import spacing\n",
    "\n",
    "app=Flask(__name__)\n",
    "\n",
    "model = Word2Vec.load('psychology.model')\n",
    "\n",
    "tagger = Mecab()\n",
    "\n",
    "text_list=[]\n",
    "with open('analysis_token.txt', 'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        text_list.append(line.rstrip())\n",
    "    \n",
    "except_words=[]\n",
    "with open('except_word.txt', 'r',encoding='utf-8') as f:\n",
    "    except_words = []\n",
    "    for line in f:\n",
    "        except_words.append(line.rstrip())\n",
    "\n",
    "josa_eomi=[]\n",
    "with open(r\"C:\\Users\\IBK\\Desktop\\josaeomi.txt\",'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        josa_eomi.append(line.rstrip())\n",
    "        \n",
    "# MAIN\n",
    "def find_error_word(): #오류가 있는지 확인\n",
    "    global error_word\n",
    "    error_word=[] #오류단어를 저장\n",
    "    for i in jamak_nouns:\n",
    "        if i not in text_list:\n",
    "            error_word.append(i)\n",
    "    return error_word\n",
    "\n",
    "\n",
    "\n",
    "def comb_error_word():\n",
    "    global err_comb\n",
    "    global err_word\n",
    "    err_word=[]\n",
    "    err_comb=[]\n",
    "    for i in range(len(error_word)):\n",
    "        for j in range(len(jamak)):\n",
    "            if error_word[i] == jamak[j]:\n",
    "                comb1=[]\n",
    "                comb2=[]\n",
    "                \n",
    "                k=0\n",
    "                while jamak[j+k] not in josa_eomi:\n",
    "                    real_err_word1=jamak[j+k]\n",
    "                    comb1.append(real_err_word1)\n",
    "                    k+=1\n",
    "                    \n",
    "                k=1\n",
    "                while jamak[j-k] not in josa_eomi:\n",
    "                    real_err_word2=jamak[j-k]\n",
    "                    comb2.insert(0,real_err_word2)\n",
    "                    k+=1\n",
    "                    \n",
    "                err_comb.append(comb2+comb1)\n",
    "                \n",
    "    \n",
    "    for i in range(len(err_comb)):\n",
    "        err_comb[i]=''.join(err_comb[i])\n",
    "    for v in err_comb:\n",
    "        if v not in err_word:\n",
    "            err_word.append(v)\n",
    "    return err_word\n",
    "\n",
    "\n",
    "def nearby_error_word(): #오류 단어 앞뒤의 3단어 뽑기\n",
    "    global check_nouns\n",
    "    check_nouns=[]#오류단어 앞뒤의 단어 저장\n",
    "    if err_word==[]:\n",
    "        return []\n",
    "    else:\n",
    "        for i in range(len(err_word)):\n",
    "            for j in range(len(line_space)):\n",
    "                if err_word[i] == line_space[j] or err_word[i] in tagger.nouns(line_space[j]) or error_word[i] in tagger.nouns(line_space[j]):\n",
    "                    check_nouns_list=[]\n",
    "                    if j == len(line_space)-1:\n",
    "                        try:\n",
    "                            li1=tagger.nouns(line_space[j-1])\n",
    "                            li2=tagger.nouns(line_space[j-2])\n",
    "                            li3=tagger.nouns(line_space[j-3])\n",
    "                            li4=tagger.nouns(line_space[j-4])\n",
    "                            li5=tagger.nouns(line_space[j-5])\n",
    "                            li6=tagger.nouns(line_space[j-6])\n",
    "                            check_nouns_list.extend(li1+li2+li3+li4+li5+li6)\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                    elif j == len(line_space)-2:\n",
    "                        try:\n",
    "                            li1=tagger.nouns(line_space[j-1])\n",
    "                            li2=tagger.nouns(line_space[j+1])\n",
    "                            li3=tagger.nouns(line_space[j-2])\n",
    "                            li4=tagger.nouns(line_space[j-3])\n",
    "                            li5=tagger.nouns(line_space[j-4])\n",
    "                            li6=tagger.nouns(line_space[j-5])\n",
    "                            check_nouns_list.extend(li1+li2+li3+li4+li5+li6)\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                    else:\n",
    "                        try:\n",
    "                            li1=tagger.nouns(line_space[j-1])\n",
    "                            li2=tagger.nouns(line_space[j+1])\n",
    "                            li3=tagger.nouns(line_space[j-2])\n",
    "                            li4=tagger.nouns(line_space[j+2])\n",
    "                            li5=tagger.nouns(line_space[j-3])\n",
    "                            li6=tagger.nouns(line_space[j-4])\n",
    "                            check_nouns_list.extend(li1+li2+li3+li4+li5+li6)\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                    check_nouns.append(check_nouns_list)    \n",
    "                    if check_nouns[0]==[]:\n",
    "                        return []\n",
    "    return check_nouns\n",
    "                                       \n",
    "def check_word_list(): #오류 단어 근처의 단어에 대해 word2vec으로 학습한 연관성 높은 단어의 리스트를 출력\n",
    "    global word_list\n",
    "    word_list=[]\n",
    "    global model_result\n",
    "    if check_nouns==[]:\n",
    "        return []\n",
    "\n",
    "    else:\n",
    "        for i in range(len(check_nouns)):\n",
    "            list_result=[]\n",
    "            for j in check_nouns[i]:\n",
    "                try:\n",
    "                    model_result=model.wv.most_similar(j,topn=100)\n",
    "                    for k in model_result:\n",
    "                        if k[0] not in except_words:\n",
    "                            list_result.append(k[0])\n",
    "                except:\n",
    "                    pass\n",
    "            word_list.append(list_result)\n",
    "            \n",
    "            if list_result==[]:\n",
    "                return []\n",
    "            \n",
    "    return word_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def romanizing(): #word_list의 한글 발음을 로마자로 변환\n",
    "    global pronounce\n",
    "    pronounce=[] # word_list의 발음을 저장.\n",
    "    if word_list == []:\n",
    "        return []\n",
    "    else:\n",
    "        for i in range(len(word_list)):\n",
    "            pronounce_list=[]\n",
    "            for j in range(len(word_list[i])):\n",
    "                try:\n",
    "                    a=Romanizer(word_list[i][j])\n",
    "                    pronounce_list.append(a.romanize())\n",
    "                except:\n",
    "                    pass\n",
    "            pronounce.append(pronounce_list)\n",
    "    return pronounce\n",
    "\n",
    "\n",
    "                                       \n",
    "\n",
    "def similarity(): #error word와 word list 단어의 발음 유사도 측정\n",
    "    global probability\n",
    "    probability=[]\n",
    "    \n",
    "    if pronounce == []:\n",
    "        return []\n",
    "    \n",
    "    else:\n",
    "        for e in range(len(err_word)):\n",
    "            a=Romanizer(err_word[e]).romanize()\n",
    "            prob=[]\n",
    "            prob1=[]\n",
    "            prob2=[]\n",
    "            prob3=[]\n",
    "            try:\n",
    "                for j in range(len(pronounce[e])):\n",
    "                    prob1.append(jellyfish.jaro_winkler_similarity(a, pronounce[e][j]))\n",
    "                    prob2.append(jellyfish.jaro_similarity(a, pronounce[e][j]))\n",
    "                    prob3.append(1-(jellyfish.levenshtein_distance(a,pronounce[e][j]))/7)\n",
    "                    prob.append(prob3[j]+(prob1[j]+prob2[j]))\n",
    "                probability.append(prob)\n",
    "            except:\n",
    "                pass\n",
    "        if pronounce[0] == []:\n",
    "            return []\n",
    "    return probability\n",
    "\n",
    "                                       \n",
    "\n",
    "def word_change(): #오류단어를 교체\n",
    "    global correct_word\n",
    "    global line_space\n",
    "    correct_word=[]\n",
    "    \n",
    "    if probability == []:\n",
    "        return line_space\n",
    "    \n",
    "    else:\n",
    "        change_word=[]\n",
    "        try:\n",
    "            for i in range(len(probability)):\n",
    "                err_word_index=probability[i].index(max(probability[i]))\n",
    "                correct_word.append(word_list[i][err_word_index])\n",
    "\n",
    "            for a in range(len(err_word)):\n",
    "                for b in range(len(line_space)):\n",
    "                    if err_word[a] == line_space[b]:\n",
    "                        line_space[b]=correct_word[a]\n",
    "        except:\n",
    "            pass\n",
    "    return line_space\n",
    "\n",
    "def word_change2():\n",
    "    global line_space\n",
    "    \n",
    "    if probability == []:\n",
    "        return line_space\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            line_space=word_change()\n",
    "            line_nnn=spacing(''.join(line_space))\n",
    "            line_nnn=tagger.morphs(line_nnn)\n",
    "            for a in range(len(err_word)):\n",
    "                for b in range(len(line_nnn)):\n",
    "                    if err_word[a] == line_nnn[b]:\n",
    "                        line_nnn[b] =correct_word[a]\n",
    "                        line_space=line_nnn\n",
    "        except:\n",
    "            pass\n",
    "                    \n",
    "    return line_space\n",
    "\n",
    "def process(line):\n",
    "    global jamak_nouns\n",
    "    global jamak\n",
    "    global line_for_space\n",
    "    global line_space\n",
    "    global change_word2\n",
    "    \n",
    "    jamak_nn=tagger.nouns(line) #자막에 나오는 의미있는 명사를 찾기\n",
    "    line_for_space=line.split(' ')\n",
    "    line_space=line_for_space.copy()\n",
    "\n",
    "    for i in range(len(line_for_space)):\n",
    "        if 2*i < len(line_for_space):\n",
    "            line_for_space.insert(2*i+1,'space')\n",
    "        \n",
    "    for i in range(len(line_for_space)):\n",
    "        line_for_space[i]=tagger.morphs(line_for_space[i])\n",
    "        \n",
    "    line_space_include=sum(line_for_space,[])\n",
    "    jamak_nn=tagger.nouns(line)\n",
    "    space_jamak=''.join(line_space_include)\n",
    "    \n",
    "    jamak_nouns=[]\n",
    "    for w in jamak_nn:\n",
    "        if w not in except_words:\n",
    "            jamak_nouns.append(w)\n",
    "\n",
    "    if tagger.morphs(space_jamak)!=[]: #자막문장을 품사별로 끊기\n",
    "        jamak=tagger.morphs(space_jamak)\n",
    "\n",
    "    error_word=find_error_word()\n",
    "    err_word=comb_error_word()\n",
    "    check_nouns=nearby_error_word()\n",
    "    word_list=check_word_list()\n",
    "    pronounce=romanizing()\n",
    "    probability=similarity()\n",
    "    change_word=word_change()\n",
    "    change_word2=word_change2()\n",
    "\n",
    "    # 자막으로 전환\n",
    "\n",
    "    final_line=''.join(change_word2)\n",
    "\n",
    "    result=spacing(final_line)\n",
    "\n",
    "    return result\n",
    "\n",
    "@app.route(\"/correctSubtitle\", methods=['POST'])\n",
    "\n",
    "def correctSubtitle():\n",
    "    params = request.get_json()\n",
    "    result=process(params['subtitle'])\n",
    "    return {'result':result}\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='127.0.0.1',port=8000,debug=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
