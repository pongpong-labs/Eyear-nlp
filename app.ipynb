{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [06/Sep/2020 12:06:49] \"\u001b[37mPOST /correctSubtitle HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [06/Sep/2020 12:08:16] \"\u001b[37mPOST /correctSubtitle HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [06/Sep/2020 12:08:40] \"\u001b[37mPOST /correctSubtitle HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from eunjeon import Mecab\n",
    "from korean_romanizer import *\n",
    "import jellyfish\n",
    "from pykospacing import spacing\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "#심리학 수업일 경우 psychology, 고체역학 수업에서 stress_strain 사용\n",
    "\n",
    "model = Word2Vec.load('stress_strain.model')\n",
    "\n",
    "tagger = Mecab()\n",
    "\n",
    "\n",
    "text_list = []\n",
    "with open('stress_strain_text.txt', 'r', encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        text_list.append(line.rstrip())\n",
    "    \n",
    "    \n",
    "except_words = []\n",
    "with open('except_word.txt', 'r', encoding = 'utf-8') as f:\n",
    "    except_words = []\n",
    "    for line in f:\n",
    "        except_words.append(line.rstrip())\n",
    "\n",
    "        \n",
    "josa_eomi = []\n",
    "with open(\"josaeomi.txt\",'r', encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        josa_eomi.append(line.rstrip())\n",
    "\n",
    "\n",
    "\n",
    "# MAIN\n",
    "def find_error_word(): #오류가 있는지 확인\n",
    "    global error_word\n",
    "    error_word = [] #오류단어를 저장\n",
    "    \n",
    "    for i in jamak_nn:\n",
    "        if i not in text_list:\n",
    "            if i in except_words:\n",
    "                pass\n",
    "            else:\n",
    "                error_word.append(i)\n",
    "            \n",
    "    return error_word\n",
    "\n",
    "\n",
    "\n",
    "def comb_error_word():\n",
    "    global err_comb\n",
    "    global err_word\n",
    "    err_word = []\n",
    "    err_comb = []\n",
    "    for i in range(len(error_word)):\n",
    "        for j in range(len(jamak)):\n",
    "            if error_word[i] == jamak[j]:\n",
    "                comb1 = []\n",
    "                comb2 = []\n",
    "                \n",
    "                k = 0\n",
    "                while jamak[j + k] not in josa_eomi:\n",
    "                    real_err_word1 = jamak[j + k]\n",
    "                    comb1.append(real_err_word1)\n",
    "                    k += 1\n",
    "                    \n",
    "                k = 1\n",
    "                while jamak[j - k] not in josa_eomi:\n",
    "                    real_err_word2 = jamak[j - k]\n",
    "                    comb2.insert(0,real_err_word2)\n",
    "                    k += 1\n",
    "                    \n",
    "                err_comb.append(comb2 + comb1)\n",
    "                \n",
    "    \n",
    "    for i in range(len(err_comb)):\n",
    "        err_comb[i] = ''.join(err_comb[i])\n",
    "    for v in err_comb:\n",
    "        if v not in err_word:\n",
    "            err_word.append(v)\n",
    "    \n",
    "    if error_word != [] and err_word == []:\n",
    "        err_word = error_word\n",
    "        return error_word\n",
    "    \n",
    "    \n",
    "    return err_word\n",
    "\n",
    "\n",
    "def nearby_error_word(): #오류 단어 앞뒤의 3단어 뽑기\n",
    "    global check_nouns\n",
    "    check_nouns = []#오류단어 앞뒤의 단어 저장\n",
    "    if err_word == []:\n",
    "        return []\n",
    "    else:\n",
    "        for i in range(len(err_word)):\n",
    "            for j in range(len(line_space)):\n",
    "                if err_word[i] == line_space[j] or err_word[i] in tagger.nouns(line_space[j]) or error_word[i] in tagger.nouns(line_space[j]):\n",
    "                    check_nouns_list = []\n",
    "                    if j == len(line_space)-1:\n",
    "                        try:\n",
    "                            li1 = tagger.nouns(line_space[j - 1])\n",
    "                            li2 = tagger.nouns(line_space[j - 2])\n",
    "                            li3 = tagger.nouns(line_space[j - 3])\n",
    "                            li4 = tagger.nouns(line_space[j - 4])\n",
    "                            li5 = tagger.nouns(line_space[j - 5])\n",
    "                            li6 = tagger.nouns(line_space[j - 6])\n",
    "                            check_nouns_list.extend(li1+li2+li3+li4+li5+li6)\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                    elif j == len(line_space) - 2:\n",
    "                        try:\n",
    "                            li1 = tagger.nouns(line_space[j - 1])\n",
    "                            li2 = tagger.nouns(line_space[j + 1])\n",
    "                            li3 = tagger.nouns(line_space[j - 2])\n",
    "                            li4 = tagger.nouns(line_space[j - 3])\n",
    "                            li5 = tagger.nouns(line_space[j - 4])\n",
    "                            li6 = tagger.nouns(line_space[j - 5])\n",
    "                            check_nouns_list.extend(li1+li2+li3+li4+li5+li6)\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                    else:\n",
    "                        try:\n",
    "                            li1 = tagger.nouns(line_space[j - 1])\n",
    "                            li2 = tagger.nouns(line_space[j + 1])\n",
    "                            li3 = tagger.nouns(line_space[j - 2])\n",
    "                            li4 = tagger.nouns(line_space[j + 2])\n",
    "                            li5 = tagger.nouns(line_space[j - 3])\n",
    "                            li6 = tagger.nouns(line_space[j - 4])\n",
    "                            check_nouns_list.extend(li1+li2+li3+li4+li5+li6)\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                    check_nouns.append(check_nouns_list)    \n",
    "                    if check_nouns[0] == []:\n",
    "                        return []\n",
    "    return check_nouns\n",
    "                                       \n",
    "def check_word_list(): #오류 단어 근처jamak의 단어에 대해 word2vec으로 학습한 연관성 높은 단어의 리스트를 출력\n",
    "    global word_list\n",
    "    word_list = []\n",
    "    global model_result\n",
    "    if check_nouns == []:\n",
    "        return []\n",
    "\n",
    "    else:\n",
    "        for i in range(len(check_nouns)):\n",
    "            list_result = []\n",
    "            for j in check_nouns[i]:\n",
    "                try:\n",
    "                    model_result = model.wv.most_similar(j, topn=200)\n",
    "                    for k in model_result:\n",
    "                        if k[0] not in except_words:\n",
    "                            list_result.append(k[0])\n",
    "                except:\n",
    "                    pass\n",
    "            word_list.append(list_result)\n",
    "            \n",
    "            \n",
    "    return word_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def romanizing(): #word_list의 한글 발음을 로마자로 변환\n",
    "    global pronounce\n",
    "    pronounce = [] # word_list의 발음을 저장.\n",
    "    if word_list == []:\n",
    "        return []\n",
    "    else:\n",
    "        for i in range(len(word_list)):\n",
    "            pronounce_list = []\n",
    "            for j in range(len(word_list[i])):\n",
    "                try:\n",
    "                    a = Romanizer(word_list[i][j])\n",
    "                    pronounce_list.append(a.romanize())\n",
    "                except:\n",
    "                    pass\n",
    "            pronounce.append(pronounce_list)\n",
    "    return pronounce\n",
    "\n",
    "\n",
    "                                       \n",
    "\n",
    "def similarity(): #error word와 word list 단어의 발음 유사도 측정\n",
    "    global probability\n",
    "    probability = []\n",
    "    \n",
    "    if pronounce == []:\n",
    "        return []\n",
    "    \n",
    "    else:\n",
    "        for e in range(len(err_word)):\n",
    "            a = Romanizer(err_word[e]).romanize()\n",
    "            prob = []\n",
    "            prob1 = []\n",
    "            prob2 = []\n",
    "            prob3 = []\n",
    "            try:\n",
    "                for j in range(len(pronounce[e])):\n",
    "                    prob1.append(jellyfish.jaro_winkler_similarity(a, pronounce[e][j]))\n",
    "                    prob2.append(jellyfish.jaro_similarity(a, pronounce[e][j]))\n",
    "                    prob3.append(1-(jellyfish.levenshtein_distance(a,pronounce[e][j]))/13)\n",
    "                    prob.append(prob3[j] + (prob1[j] + prob2[j]))\n",
    "                probability.append(prob)\n",
    "            except:\n",
    "                pass\n",
    "        if pronounce[0] == []:\n",
    "            return []\n",
    "    return probability\n",
    "\n",
    "                                       \n",
    "def word_change(): #오류단어를 교체\n",
    "    global correct_word\n",
    "    global line_space\n",
    "\n",
    "    correct_word = []\n",
    "    \n",
    "    if probability == []:\n",
    "        return line_space\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            for i in range(len(probability)):\n",
    "                err_word_index = probability[i].index(max(probability[i]))\n",
    "                correct_word.append(word_list[i][err_word_index])\n",
    "\n",
    "            for a in range(len(err_word)):\n",
    "                for b in range(len(line_space)):\n",
    "                    if err_word[a] == line_space[b]:\n",
    "                        line_space[b] = correct_word[a]\n",
    "        except:\n",
    "            pass\n",
    "    return line_space\n",
    "\n",
    "def word_change_again():\n",
    "    global line_space\n",
    "    \n",
    "    if probability == []:\n",
    "        return line_space\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            line_space = change_word\n",
    "            line_nnn = spacing(''.join(line_space))\n",
    "            line_nnn = tagger.morphs(line_nnn)\n",
    "            for a in range(len(err_word)):\n",
    "                for b in range(len(line_nnn)):\n",
    "                    if err_word[a] == line_nnn[b]:\n",
    "                        line_nnn[b] = correct_word[a]\n",
    "                        line_space = line_nnn\n",
    "                    if error_word[a] == line_nnn[b]:\n",
    "                        line_nnn[b] = correct_word[a]\n",
    "                        line_space = line_nnn\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "                    \n",
    "    return line_space\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process(line):\n",
    "    global jamak_nouns\n",
    "    global jamak\n",
    "    global jamak_nn\n",
    "    global line_for_space\n",
    "    global line_space\n",
    "    global change_word, change_word_again\n",
    "    global result\n",
    "    \n",
    "    \n",
    "    line_li=list(line)\n",
    "    \n",
    "    for i in range(len(line_li)-1):\n",
    "        if line_li[i] == '제':\n",
    "            if line_li[i+1] == ' ' and line_li[i-1] == ' ':\n",
    "                line_li[i] = '이제'\n",
    "\n",
    "    for i in range(len(line_li)-1):\n",
    "        if line_li[i] == '자':\n",
    "            if line_li[i+1] == ' ' and line_li[i-1] == ' ':\n",
    "                line_li[i] = ' '\n",
    "\n",
    "\n",
    "    line=''.join(line_li)\n",
    "\n",
    "    jamak_nn = tagger.nouns(line) #자막에 나오는 의미있는 명사를 찾기\n",
    "    jamak_morphs = tagger.morphs(line)\n",
    "    line_for_space = line.split(' ')\n",
    "    line_space = line_for_space.copy()\n",
    "    \n",
    "    \n",
    "    for i in range(len(line_for_space)):\n",
    "        if 2 * i < len(line_for_space):\n",
    "            line_for_space.insert(2 * i + 1, 'space')\n",
    "\n",
    "    for i in range(len(line_for_space)):\n",
    "        line_for_space[i] = tagger.morphs(line_for_space[i])\n",
    "\n",
    "    line_space_include = sum(line_for_space,[])\n",
    "    jamak_nn = tagger.nouns(line)\n",
    "    space_jamak = ''.join(line_space_include)\n",
    "\n",
    "    jamak_nouns=[]\n",
    "    for w in jamak_nn:\n",
    "        if w not in except_words:\n",
    "            jamak_nouns.append(w)\n",
    "            \n",
    "    if tagger.morphs(space_jamak) != []: #자막문장을 품사별로 끊기\n",
    "        jamak = tagger.morphs(space_jamak)\n",
    "\n",
    "    \n",
    "\n",
    "    error_word = find_error_word()\n",
    "    err_word = comb_error_word()\n",
    "    check_nouns = nearby_error_word()\n",
    "    word_list = check_word_list()\n",
    "    pronounce = romanizing()\n",
    "    probability = similarity()\n",
    "    change_word = word_change()\n",
    "    change_word_again = word_change_again()\n",
    "    # 자막으로 전환\n",
    "\n",
    "    final_line = ''.join(change_word_again)\n",
    "    \n",
    "    errnum_convlist=['개','명','시','분','초','원','달','불','마','수','승','센','미','키','단','번','째']\n",
    "\n",
    "    fi_li=tagger.morphs(final_line)\n",
    "\n",
    "    for i in range(len(fi_li)-1):\n",
    "        if fi_li[i] == '4':\n",
    "            if fi_li[i+1] not in errnum_convlist:\n",
    "                fi_li[i] = ' 네 '\n",
    "        elif fi_li[len(fi_li)-1] == '4':\n",
    "            fi_li[(len(fi_li)-1)] = ' 네 '\n",
    "\n",
    "    for i in range(len(fi_li)-1):\n",
    "        if fi_li[i] == '5':\n",
    "            if fi_li[i+1] not in errnum_convlist:\n",
    "                fi_li[i] = ' 오 '\n",
    "        elif fi_li[len(fi_li)-1] == '5':\n",
    "            fi_li[(len(fi_li)-1)] = ' 오 '\n",
    "\n",
    "    fi_line=''.join(fi_li)\n",
    "    fi_line=list(fi_line)\n",
    "\n",
    "    for i in range(len(fi_line)-1):\n",
    "        if fi_line[i] == '동':\n",
    "            if fi_line[i+1] == '기':\n",
    "                if fi_line[i-1] == ' ':\n",
    "                    pass\n",
    "                else:\n",
    "                    fi_line.insert(i,' ')\n",
    "\n",
    "    for i in range(len(line)-1):\n",
    "        if line_li[i] == '예':\n",
    "            if line_li[i+1] == ' ' and line_li[i-1] == ' ':\n",
    "                for j in range(len(fi_line)-1):\n",
    "                    if fi_line[j] == '예':\n",
    "                        fi_line.insert(j+1, ' ')\n",
    "\n",
    "    fi_line=''.join(fi_line)\n",
    "\n",
    "    result = spacing(fi_line)\n",
    "\n",
    "    return result\n",
    "\n",
    "@app.route(\"/correctSubtitle\", methods=['POST'])\n",
    "\n",
    "def correctSubtitle():\n",
    "    params = request.get_json()\n",
    "    result = process(params['subtitle'])\n",
    "    return {'result':result}\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=8000,debug=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
