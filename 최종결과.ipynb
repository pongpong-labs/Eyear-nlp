{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from eunjeon import Mecab\n",
    "from korean_romanizer import *\n",
    "import jellyfish\n",
    "from pykospacing import spacing\n",
    "\n",
    "tagger = Mecab()\n",
    "\n",
    "model = Word2Vec.load('psychology.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=[]\n",
    "with open('analysis_token.txt', 'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        text_list.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "except_words=[]\n",
    "with open('except_word.txt', 'r',encoding='utf-8') as f:\n",
    "    except_words = []\n",
    "    for line in f:\n",
    "        except_words.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# MAIN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_error_word(): #오류가 있는지 확인\n",
    "    error_word=[] #오류단어를 저장\n",
    "    for i in jamak_nouns:\n",
    "        if i not in text_list:\n",
    "            error_word.append(i)\n",
    "    return error_word\n",
    "\n",
    "def nearby_error_word(): #오류 단어 앞뒤의 3단어 뽑기\n",
    "    global check_nouns\n",
    "    check_nouns=[]#오류단어 앞뒤의 단어 저장\n",
    "    if error_word==[]:\n",
    "        return []\n",
    "    else:\n",
    "        for i in range(len(error_word)):\n",
    "            for j in range(len(jamak_nouns)):\n",
    "                if error_word[i] == jamak_nouns[j]:\n",
    "                    check_nouns_list=[]\n",
    "                    if j == len(jamak_nouns)-1:\n",
    "                        try:\n",
    "                            check_nouns_list.append(jamak_nouns[j-1])\n",
    "                            check_nouns_list.append(jamak_nouns[j-2])\n",
    "                            check_nouns_list.append(jamak_nouns[j-3])\n",
    "                            check_nouns_list.append(jamak_nouns[j-4])\n",
    "                        except:\n",
    "                            pass\n",
    "                    elif j == len(jamak_nouns)-2:\n",
    "                        try:\n",
    "                            check_nouns_list.append(jamak_nouns[j-1])\n",
    "                            check_nouns_list.append(jamak_nouns[j+1])\n",
    "                            check_nouns_list.append(jamak_nouns[j-2])\n",
    "                            check_nouns_list.append(jamak_nouns[j-3])\n",
    "                        except:\n",
    "                            pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            check_nouns_list.append(jamak_nouns[j-1])\n",
    "                            check_nouns_list.append(jamak_nouns[j+1])\n",
    "                            check_nouns_list.append(jamak_nouns[j-2])\n",
    "                            check_nouns_list.append(jamak_nouns[j+2])\n",
    "                        except:\n",
    "                            pass\n",
    "                    check_nouns.append(check_nouns_list)    \n",
    "    return check_nouns\n",
    "\n",
    "def check_word_list(): #오류 단어 근처의 단어에 대해 word2vec으로 학습한 연관성 높은 단어의 리스트를 출력\n",
    "    word_list=[]\n",
    "    global model_result\n",
    "    if check_nouns==[]:\n",
    "        return []\n",
    "\n",
    "    else:\n",
    "        for i in range(len(check_nouns)):\n",
    "            list_result=[]\n",
    "            for j in check_nouns[i]:\n",
    "                try:\n",
    "                    model_result=model.wv.most_similar(j,topn=100)\n",
    "                    for k in model_result:\n",
    "                        if k[0] not in except_words:\n",
    "                            list_result.append(k[0])\n",
    "                except:\n",
    "                    pass\n",
    "            word_list.append(list_result)\n",
    "    \n",
    "    return word_list\n",
    "\n",
    "\n",
    "def romanizing(): #word_list의 한글 발음을 로마자로 변환\n",
    "    global pronounce\n",
    "    pronounce=[] # word_list의 발음을 저장.\n",
    "    if word_list == []:\n",
    "        return []\n",
    "    else:\n",
    "        for i in range(len(word_list)):\n",
    "            pronounce_list=[]\n",
    "            for j in range(len(word_list[i])):\n",
    "                try:\n",
    "                    a=Romanizer(word_list[i][j])\n",
    "                    pronounce_list.append(a.romanize())\n",
    "                except:\n",
    "                    pass\n",
    "            pronounce.append(pronounce_list)\n",
    "    return pronounce\n",
    "\n",
    "\n",
    "def similarity(): #error word와 word list 단어의 발음 유사도 측정\n",
    "    probability=[]\n",
    "    if pronounce == []:\n",
    "        return []\n",
    "    else:\n",
    "        for e in range(len(error_word)):\n",
    "            a=Romanizer(error_word[e]).romanize()\n",
    "            prob=[]\n",
    "            prob1=[]\n",
    "            prob2=[]\n",
    "            prob3=[]\n",
    "            for j in range(len(pronounce[e])):\n",
    "                prob1.append(jellyfish.jaro_winkler_similarity(a, pronounce[e][j]))\n",
    "                prob2.append(jellyfish.jaro_similarity(a, pronounce[e][j]))\n",
    "                prob3.append(1-(jellyfish.levenshtein_distance(a,pronounce[e][j]))/7)\n",
    "                prob.append(prob3[j]+(prob1[j]+prob2[j])/2)\n",
    "            probability.append(prob)\n",
    "    return probability\n",
    "\n",
    "\n",
    "\n",
    "def word_change(): #오류단어를 교체\n",
    "    global correct_word\n",
    "    correct_word=[]\n",
    "    if probability==[]:\n",
    "        return jamak\n",
    "    else:\n",
    "        change_word=[]\n",
    "        for i in range(len(probability)):\n",
    "            err_word_index=probability[i].index(max(probability[i]))\n",
    "            correct_word.append(word_list[i][err_word_index])\n",
    "            \n",
    "        for a in range(len(jamak)):\n",
    "            for b in range(len(error_word)):\n",
    "                if jamak[a] == error_word[b]:\n",
    "                    jamak[a]=correct_word[b]\n",
    "    return jamak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우리 교재에 있는 것처럼 사물을 지각하는 것은 감각기관을 통해서 경험에 의해서 그 외 우리가 지각이 내지 상응적 태어날 때부터 갖고 태어나게 되는 건 아니다 이렇게 이야기하는 \n",
      "\n",
      "사물의 지각에 대한 그런 연구들이 있었습니다고 민들이 있었던 거죠 \n",
      "\n",
      "그 다음에 이제 심리학에 그 영향을 입은 생리학적인 연계생리학적 영향이다고 하는 것은 이제 사람에 대해서 \n",
      "\n",
      "사람의 신체 그리고 이제 사람의 어떤 그 귀능 특별히 여기서 이야기할 때 뇌라고 하는 것을 대단히 중요하게 생각을 하구요 \n",
      "\n",
      "그 다음에 이제 우리가 심리적인 영향을 주는 사람의 신체는 호르몬이라고 하는 것들이 중요하게 생각합니다 \n",
      "\n",
      "그러니까 이 호르몬이 몸에서 움직이면 호르몬이구요똑같은 물질이 우리의 뇌로 들어가면 신경전달물질 \n",
      "\n",
      "이렇게 뇌세포네 세포 사이를 연결해주는 신경전달물질이라고 하는 걸로 이렇게 분리됩니다 \n",
      "\n",
      "똑같은 똑같은 이제 그쉽게 말하면 똑같은 기능을 하는 건데 몸에서 기능하면 호르몬 뇌에서 기능하면 신경전달을 \n",
      "\n",
      "이렇게 똑같은 걸 갖고 이렇게 명칭도 다르게 부르는 그런 특성들이 있습니다 \n",
      "\n",
      "왜 그렇게 부르냐면 뭐라고 부르면 이건 몸에서 기능하는 거구나 뭐라고 부르면 아이게 뇌에서 기능하는 거구나 음 \n",
      "\n",
      "이렇게 구분하기 위해서 그렇게 부르는 연 \n",
      "\n",
      "그러니까 이제 이런 것들이 이제 전체적으로 사람의 몸에 대한 과학적인 연구가 증가하면 증가할수록 \n",
      "\n",
      "사람의 심리를 이해하는 일도 훨씬 더 영향을 많이 줍니다 해서 그 영향으로 인해서 우리가 이제 어떤 뭐 \n",
      "\n",
      "심리적인 문제가 장흔한 건데 우울하다 이러면 병원에 가서 약소음 우울한 기분 기운이 딱 없어지는 \n",
      "\n",
      "이런 한 것들이 이제 이런 그 생리학적인 연구를 통해서 나온 결과물이다라고 보시면 됩니다 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f=open(r'C:\\Users\\IBK\\Desktop\\psychology_lecture.txt',encoding=\"utf8\") # 자막이 들어올 위치. 자막을 한 줄씩 받을 예정\n",
    "\n",
    "while True:\n",
    "    line=f.readline()\n",
    "    jamak_nn=tagger.nouns(line) #자막에 나오는 의미있는 명사를 찾기\n",
    "    \n",
    "    if not line:\n",
    "        break\n",
    "    \n",
    "    jamak_nouns=[]\n",
    "    for w in jamak_nn:\n",
    "        if w not in except_words:\n",
    "            jamak_nouns.append(w)\n",
    "    \n",
    "    if tagger.morphs(line)!=[]: #자막문장을 품사별로 끊기\n",
    "        jamak=tagger.morphs(line)\n",
    "        \n",
    "    error_word=find_error_word()\n",
    "    check_nouns=nearby_error_word()\n",
    "    word_list=check_word_list()\n",
    "    pronounce=romanizing()\n",
    "    probability=similarity()\n",
    "    change_word=word_change()\n",
    "    jamak=''.join(jamak)\n",
    "    print(spacing(jamak),'\\n')\n",
    "    \n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
