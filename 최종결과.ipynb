{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from eunjeon import Mecab\n",
    "from korean_romanizer import *\n",
    "import jellyfish\n",
    "from pykospacing import spacing\n",
    "\n",
    "\n",
    "model = Word2Vec.load('stress_strain.model')\n",
    "\n",
    "tagger = Mecab()\n",
    "\n",
    "\n",
    "text_list = []\n",
    "with open('stress_strain_text.txt', 'r', encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        text_list.append(line.rstrip())\n",
    "    \n",
    "    \n",
    "except_words = []\n",
    "with open('except_word.txt', 'r', encoding = 'utf-8') as f:\n",
    "    except_words = []\n",
    "    for line in f:\n",
    "        except_words.append(line.rstrip())\n",
    "\n",
    "        \n",
    "josa_eomi = []\n",
    "with open(\"josaeomi.txt\",'r', encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        josa_eomi.append(line.rstrip())\n",
    "\n",
    "\n",
    "\n",
    "# MAIN\n",
    "def find_error_word(): #오류가 있는지 확인\n",
    "    global error_word\n",
    "    error_word = [] #오류단어를 저장\n",
    "    \n",
    "    for i in jamak_nn:\n",
    "        if i not in text_list:\n",
    "            if i in except_words:\n",
    "                pass\n",
    "            else:\n",
    "                error_word.append(i)\n",
    "            \n",
    "    return error_word\n",
    "\n",
    "\n",
    "\n",
    "def comb_error_word():\n",
    "    global err_comb\n",
    "    global err_word\n",
    "    err_word = []\n",
    "    err_comb = []\n",
    "    for i in range(len(error_word)):\n",
    "        for j in range(len(jamak)):\n",
    "            if error_word[i] == jamak[j]:\n",
    "                comb1 = []\n",
    "                comb2 = []\n",
    "                \n",
    "                k = 0\n",
    "                while jamak[j + k] not in josa_eomi:\n",
    "                    real_err_word1 = jamak[j + k]\n",
    "                    comb1.append(real_err_word1)\n",
    "                    k += 1\n",
    "                    \n",
    "                k = 1\n",
    "                while jamak[j - k] not in josa_eomi:\n",
    "                    real_err_word2 = jamak[j - k]\n",
    "                    comb2.insert(0,real_err_word2)\n",
    "                    k += 1\n",
    "                    \n",
    "                err_comb.append(comb2 + comb1)\n",
    "                \n",
    "    \n",
    "    for i in range(len(err_comb)):\n",
    "        err_comb[i] = ''.join(err_comb[i])\n",
    "    for v in err_comb:\n",
    "        if v not in err_word:\n",
    "            err_word.append(v)\n",
    "    \n",
    "    if error_word != [] and err_word == []:\n",
    "        err_word = error_word\n",
    "        return error_word\n",
    "    \n",
    "    \n",
    "    return err_word\n",
    "\n",
    "\n",
    "def nearby_error_word(): #오류 단어 앞뒤의 3단어 뽑기\n",
    "    global check_nouns\n",
    "    check_nouns = []#오류단어 앞뒤의 단어 저장\n",
    "    if err_word == []:\n",
    "        return []\n",
    "    else:\n",
    "        for i in range(len(err_word)):\n",
    "            for j in range(len(line_space)):\n",
    "                if err_word[i] == line_space[j] or err_word[i] in tagger.nouns(line_space[j]) or error_word[i] in tagger.nouns(line_space[j]):\n",
    "                    check_nouns_list = []\n",
    "                    if j == len(line_space)-1:\n",
    "                        try:\n",
    "                            li1 = tagger.nouns(line_space[j - 1])\n",
    "                            li2 = tagger.nouns(line_space[j - 2])\n",
    "                            li3 = tagger.nouns(line_space[j - 3])\n",
    "                            li4 = tagger.nouns(line_space[j - 4])\n",
    "                            li5 = tagger.nouns(line_space[j - 5])\n",
    "                            li6 = tagger.nouns(line_space[j - 6])\n",
    "                            check_nouns_list.extend(li1+li2+li3+li4+li5+li6)\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                    elif j == len(line_space) - 2:\n",
    "                        try:\n",
    "                            li1 = tagger.nouns(line_space[j - 1])\n",
    "                            li2 = tagger.nouns(line_space[j + 1])\n",
    "                            li3 = tagger.nouns(line_space[j - 2])\n",
    "                            li4 = tagger.nouns(line_space[j - 3])\n",
    "                            li5 = tagger.nouns(line_space[j - 4])\n",
    "                            li6 = tagger.nouns(line_space[j - 5])\n",
    "                            check_nouns_list.extend(li1+li2+li3+li4+li5+li6)\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                    else:\n",
    "                        try:\n",
    "                            li1 = tagger.nouns(line_space[j - 1])\n",
    "                            li2 = tagger.nouns(line_space[j + 1])\n",
    "                            li3 = tagger.nouns(line_space[j - 2])\n",
    "                            li4 = tagger.nouns(line_space[j + 2])\n",
    "                            li5 = tagger.nouns(line_space[j - 3])\n",
    "                            li6 = tagger.nouns(line_space[j - 4])\n",
    "                            check_nouns_list.extend(li1+li2+li3+li4+li5+li6)\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                    check_nouns.append(check_nouns_list)    \n",
    "                    if check_nouns[0] == []:\n",
    "                        return []\n",
    "    return check_nouns\n",
    "                                       \n",
    "def check_word_list(): #오류 단어 근처jamak의 단어에 대해 word2vec으로 학습한 연관성 높은 단어의 리스트를 출력\n",
    "    global word_list\n",
    "    word_list = []\n",
    "    global model_result\n",
    "    if check_nouns == []:\n",
    "        return []\n",
    "\n",
    "    else:\n",
    "        for i in range(len(check_nouns)):\n",
    "            list_result = []\n",
    "            for j in check_nouns[i]:\n",
    "                try:\n",
    "                    model_result = model.wv.most_similar(j, topn=200)\n",
    "                    for k in model_result:\n",
    "                        if k[0] not in except_words:\n",
    "                            list_result.append(k[0])\n",
    "                except:\n",
    "                    pass\n",
    "            word_list.append(list_result)\n",
    "            \n",
    "            \n",
    "    return word_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def romanizing(): #word_list의 한글 발음을 로마자로 변환\n",
    "    global pronounce\n",
    "    pronounce = [] # word_list의 발음을 저장.\n",
    "    if word_list == []:\n",
    "        return []\n",
    "    else:\n",
    "        for i in range(len(word_list)):\n",
    "            pronounce_list = []\n",
    "            for j in range(len(word_list[i])):\n",
    "                try:\n",
    "                    a = Romanizer(word_list[i][j])\n",
    "                    pronounce_list.append(a.romanize())\n",
    "                except:\n",
    "                    pass\n",
    "            pronounce.append(pronounce_list)\n",
    "    return pronounce\n",
    "\n",
    "\n",
    "                                       \n",
    "\n",
    "def similarity(): #error word와 word list 단어의 발음 유사도 측정\n",
    "    global probability\n",
    "    probability = []\n",
    "    \n",
    "    if pronounce == []:\n",
    "        return []\n",
    "    \n",
    "    else:\n",
    "        for e in range(len(err_word)):\n",
    "            a = Romanizer(err_word[e]).romanize()\n",
    "            prob = []\n",
    "            prob1 = []\n",
    "            prob2 = []\n",
    "            prob3 = []\n",
    "            try:\n",
    "                for j in range(len(pronounce[e])):\n",
    "                    prob1.append(jellyfish.jaro_winkler_similarity(a, pronounce[e][j]))\n",
    "                    prob2.append(jellyfish.jaro_similarity(a, pronounce[e][j]))\n",
    "                    prob3.append(1-(jellyfish.levenshtein_distance(a,pronounce[e][j]))/13)\n",
    "                    prob.append(prob3[j] + (prob1[j] + prob2[j]))\n",
    "                probability.append(prob)\n",
    "            except:\n",
    "                pass\n",
    "        if pronounce[0] == []:\n",
    "            return []\n",
    "    return probability\n",
    "\n",
    "                                       \n",
    "def word_change(): #오류단어를 교체\n",
    "    global correct_word\n",
    "    global line_space\n",
    "\n",
    "    correct_word = []\n",
    "    \n",
    "    if probability == []:\n",
    "        return line_space\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            for i in range(len(probability)):\n",
    "                err_word_index = probability[i].index(max(probability[i]))\n",
    "                correct_word.append(word_list[i][err_word_index])\n",
    "\n",
    "            for a in range(len(err_word)):\n",
    "                for b in range(len(line_space)):\n",
    "                    if err_word[a] == line_space[b]:\n",
    "                        line_space[b] = correct_word[a]\n",
    "        except:\n",
    "            pass\n",
    "    return line_space\n",
    "\n",
    "def word_change_again():\n",
    "    global line_space\n",
    "    \n",
    "    if probability == []:\n",
    "        return line_space\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            line_space = change_word\n",
    "            line_nnn = spacing(''.join(line_space))\n",
    "            line_nnn = tagger.morphs(line_nnn)\n",
    "            for a in range(len(err_word)):\n",
    "                for b in range(len(line_nnn)):\n",
    "                    if err_word[a] == line_nnn[b]:\n",
    "                        line_nnn[b] = correct_word[a]\n",
    "                        line_space = line_nnn\n",
    "                    if error_word[a] == line_nnn[b]:\n",
    "                        line_nnn[b] = correct_word[a]\n",
    "                        line_space = line_nnn\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "                    \n",
    "    return line_space\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요건 이제 그 0강 연안 재료 연강 우리가 주변에서 흔히 볼 수 있는\n",
      "['요', '건', ' ', '이', '제', ' ', '그', ' ', '0', '강', ' ', '연', '안', ' ', '재', '료', ' ', '연', '강', ' ', '우', '리', '가', ' ', '주', '변', '에', '서', ' ', '흔', '히', ' ', '볼', ' ', '수', ' ', '있', '는']\n",
      "line_space =  ['요건', '이제', '그', '0강', '연안', '재료', '연강', '우리가', '주변에서', '흔히', '볼', '수', '있는']\n",
      "['강', '연안', '재료', '연강', '주변', '수']\n",
      "error_word =  ['연안']\n",
      "err_word =  ['연안']\n",
      "check_nouns =  [['강', '재료', '연강', '요건']]\n",
      "word_list =  [['판단', '강의', '열처리', '파악', '범위', '평가', '접합', '상수', '피로', '반면', '경향', '신뢰', '력', '진폭', '시', '이하', '구분', '종류', '관', '알루미늄', '초과', '구조', '확인', '시험기', '허용', '안', '변', '탄소성', '과정', '유리', '영률', '중요', '표준', '평판', '특성', '연결', '한편', '데', '식', '이론', '그림', '현상', '진응', '주철', '목적', '활용', '한쪽', '가공', '장강', '계통', '대부분', '연신', '절차', '세라믹', '장력', '지속', '부정정', '학습', '플랜지', '재하', '보통', '고찰', '집중', '백분율', '금속', '강성', '수나사', '연신율', '고유', '경우', '선택', '실제', '차', '충격', '원', '영향', '단순', '표', '균열', '지지', '운전', '두께', '기계', '잔류', '푸아송', '눈금', '변형도', '조건', '복합', '암나사', '제조', '송', '특정', '재질', '말', '량', '대응', '문제', '의미', '파손', '관찰', '튜브', '역', '연강', '도달', '수평', '국부', '극한', '실험', '후크', '유지', '아래', '폭', '스트레인', '결정', '수', '강판', '장량', '가로', '구간', '예제', '훅', '잉여', '지력', '최초', '수식', '강봉', '미', '인장시험', '응답', '고정', '원리', '축선', '치수', '전달', '예측', '중앙', '비', '복점', '강관', '기초', '시작', '압입', '위쪽', '분리', '직각', '능력', '부', '신장', '게이지', '성질', '저항', '가지', '표시', '계수', '상황', '횡단면', '일정', '접촉', '차이', '인', '효과', '냉간', '중간', '음', '시트', '정량', '간', '직경', '구성', '기준', '각도', '초기', '양쪽', '포함', '축하', '항복', '양', '물', '모양', '회전', '일치', '영구', '항복점', '값', '정역학', '표현', '압', '표시법', '제거', '최대', '연성', '경도', '단계', '연성', '유리', '인성', '성질', '취성', '능력', '균열', '현상', '항복점', '주철', '영구', '량', '대부분', '금속', '특성', '충격', '강성', '의미', '알루미늄', '파괴', '세라믹', '외력', '이하', '극한', '구분', '구조', '시작', '최대', '가공', '고유', '대응', '기계', '도달', '푸아송', '강재', '연신율', '파악', '종류', '과정', '보통', '범위', '탄소성', '집중', '목적', '유지', '감소', '표준', '후크', '제조', '특정', '장력', '피로', '이론', '장량', '재질', '활용', '중요', '시', '재하', '백분율', '한쪽', '지속', '두께', '데', '허용', '말', '평판', '고찰', '원래', '관찰', '연신', '비', '파손', '저항', '초기', '단순', '초과', '구간', '수나사', '선택', '장강', '증가', '치수', '열처리', '잔류', '인장', '후', '계수', '튜브', '원', '시험기', '강봉', '압', '부정정', '접합', '관', '연결', '진폭', '경향', '신장', '표시', '가로', '제거', '중', '기준', '고정', '축선', '지지', '위쪽', '예제', '관계', '포함', '인', '폭', '중간', '압입', '역', '실험', '상수', '수축', '냉간', '국부', '강', '강관', '가지', '아래', '암나사', '표', '수식', '플랜지', '음', '최초', '상황', '실제', '정량', '표현', '학습', '효과', '계통', '눈금', '평가', '양쪽', '경도', '부', '미', '반면', '훅', '변형도', '직각', '수평', '지력', '수', '탄성', '분리', '문제', '복점', '력', '복합', '송', '각도', '접촉', '구성', '표시법', '회전', '강판', '전달', '중앙', '양', '물', '형태', '직경', '변', '항복', '표기', '간', '시트', '축하', '파단', '원리', '강의', '일정', '잉여', '장', '전', '변형률', '위', '횡단면', '평균', '직교', '소성', '한도', '직선', '판단', '접합', '경향', '신뢰', '력', '반면', '시', '안', '절차', '진응', '과정', '평가', '파악', '상수', '강의', '열처리', '알루미늄', '식', '변', '이하', '데', '관', '피로', '구분', '범위', '그림', '확인', '진폭', '플랜지', '실제', '한쪽', '특성', '한편', '시험기', '표준', '주철', '가공', '구조', '종류', '역', '활용', '연결', '운전', '세라믹', '고찰', '선택', '경우', '눈금', '연신율', '계통', '수나사', '허용', '장력', '초과', '지속', '강', '재하', '잔류', '중요', '관찰', '문제', '재질', '유리', '제조', '국부', '목적', '최초', '강성', '탄소성', '수식', '현상', '금속', '장강', '량', '극한', '백분율', '축선', '영률', '두께', '튜브', '연신', '결정', '파손', '이론', '보통', '도달', '영향', '집중', '복합', '표', '고유', '암나사', '스트레인', '원리', '충격', '장량', '대부분', '의미', '폭', '대응', '학습', '평판', '압입', '후크', '변형도', '인장시험', '중간', '조건', '원', '지지', '예측', '게이지', '전달', '중앙', '가로', '단순', '치수', '차이', '부정정', '특정', '실험', '강봉', '차', '상황', '수', '수평', '양', '지력', '유지', '효과', '훅', '경도', '축하', '일정', '예제', '기계', '말', '균열', '분리', '일치', '위쪽', '미', '잉여', '푸아송', '강판', '음', '아래', '횡단면', '양쪽', '복점', '신장', '직각', '비', '간', '부', '정량', '냉간', '강관', '응답', '포함', '필요', '저항', '가지', '선', '접촉', '송', '값', '기준', '구성', '초기', '표시', '능력', '고정', '회전', '링크', '직경', '인', '각도', '표시법', '기초', '이용', '물', '작업', '시작', '압', '구간', '합력', '장', '지점', '수축', '정역학', '직교', '시트']]\n",
      "pronounce =  [['pandan', 'gangui', 'yeolcheori', 'paak', 'beomwi', 'pyeongga', 'jeophap', 'sangsu', 'piro', 'banmyeon', 'gyeonghyang', 'sinroe', 'ryeok', 'jinpok', 'si', 'iha', 'gubun', 'jongryu', 'gwan', 'alruminyum', 'chogwa', 'gujo', 'hwakin', 'siheomgi', 'heoyong', 'an', 'byeon', 'tansoseong', 'gwajeong', 'yuri', 'yeongryul', 'jungyo', 'pyojun', 'pyeongpan', 'teukseong', 'yeongyeol', 'hanpyeon', 'de', 'sik', 'iron', 'geurim', 'hyeonsang', 'jineung', 'jucheol', 'mokjeok', 'hwalyong', 'hanjjok', 'gagong', 'janggang', 'gyetong', 'daebubun', 'yeonsin', 'jeolcha', 'seramik', 'jangryeok', 'jisok', 'bujeongjeong', 'hakseup', 'peulraenji', 'jaeha', 'botong', 'gochal', 'jipjung', 'baekbunyul', 'geumsok', 'gangseong', 'sunasa', 'yeonsinyul', 'goyu', 'gyeongu', 'seontaek', 'silje', 'cha', 'chunggyeok', 'won', 'yeonghyang', 'dansun', 'pyo', 'gyunyeol', 'jiji', 'unjeon', 'dukke', 'gigye', 'janryu', 'puasong', 'nungeum', 'byeonhyeongdo', 'jogeon', 'bokhap', 'amnasa', 'jejo', 'song', 'teukjeong', 'jaejil', 'mal', 'ryang', 'daeeung', 'munje', 'uimi', 'pason', 'gwanchal', 'tyubeu', 'yeok', 'yeongang', 'dodal', 'supyeong', 'gukbu', 'geukhan', 'silheom', 'hukeu', 'yuji', 'arae', 'pok', 'seuteurein', 'gyeoljeong', 'su', 'gangpan', 'jangryang', 'garo', 'gugan', 'yeje', 'huk', 'ingyeo', 'jiryeok', 'choecho', 'susik', 'gangbong', 'mi', 'injangsiheom', 'eungdap', 'gojeong', 'wonri', 'chukseon', 'chisu', 'jeondal', 'yecheuk', 'jungang', 'bi', 'bokjeom', 'ganggwan', 'gicho', 'sijak', 'apip', 'wijjok', 'bunri', 'jikgak', 'neungryeok', 'bu', 'sinjang', 'geiji', 'seongjil', 'jeohang', 'gaji', 'pyosi', 'gyesu', 'sanghwang', 'hoengdanmyeon', 'iljeong', 'jeopchok', 'chai', 'in', 'hyogwa', 'naenggan', 'junggan', 'eum', 'siteu', 'jeongryang', 'gan', 'jikgyeong', 'guseong', 'gijun', 'gakdo', 'chogi', 'yangjjok', 'poham', 'chukha', 'hangbok', 'yang', 'mul', 'moyang', 'hoejeon', 'ilchi', 'yeonggu', 'hangbokjeom', 'gap', 'jeongyeokhak', 'pyohyeon', 'ap', 'pyosibeop', 'jegeo', 'choedae', 'yeonseong', 'gyeongdo', 'dangye', 'yeonseong', 'yuri', 'inseong', 'seongjil', 'chwiseong', 'neungryeok', 'gyunyeol', 'hyeonsang', 'hangbokjeom', 'jucheol', 'yeonggu', 'ryang', 'daebubun', 'geumsok', 'teukseong', 'chunggyeok', 'gangseong', 'uimi', 'alruminyum', 'pagoe', 'seramik', 'oeryeok', 'iha', 'geukhan', 'gubun', 'gujo', 'sijak', 'choedae', 'gagong', 'goyu', 'daeeung', 'gigye', 'dodal', 'puasong', 'gangjae', 'yeonsinyul', 'paak', 'jongryu', 'gwajeong', 'botong', 'beomwi', 'tansoseong', 'jipjung', 'mokjeok', 'yuji', 'gamso', 'pyojun', 'hukeu', 'jejo', 'teukjeong', 'jangryeok', 'piro', 'iron', 'jangryang', 'jaejil', 'hwalyong', 'jungyo', 'si', 'jaeha', 'baekbunyul', 'hanjjok', 'jisok', 'dukke', 'de', 'heoyong', 'mal', 'pyeongpan', 'gochal', 'wonrae', 'gwanchal', 'yeonsin', 'bi', 'pason', 'jeohang', 'chogi', 'dansun', 'chogwa', 'gugan', 'sunasa', 'seontaek', 'janggang', 'jeungga', 'chisu', 'yeolcheori', 'janryu', 'injang', 'hu', 'gyesu', 'tyubeu', 'won', 'siheomgi', 'gangbong', 'ap', 'bujeongjeong', 'jeophap', 'gwan', 'yeongyeol', 'jinpok', 'gyeonghyang', 'sinjang', 'pyosi', 'garo', 'jegeo', 'jung', 'gijun', 'gojeong', 'chukseon', 'jiji', 'wijjok', 'yeje', 'gwangye', 'poham', 'in', 'pok', 'junggan', 'apip', 'yeok', 'silheom', 'sangsu', 'suchuk', 'naenggan', 'gukbu', 'gang', 'ganggwan', 'gaji', 'arae', 'amnasa', 'pyo', 'susik', 'peulraenji', 'eum', 'choecho', 'sanghwang', 'silje', 'jeongryang', 'pyohyeon', 'hakseup', 'hyogwa', 'gyetong', 'nungeum', 'pyeongga', 'yangjjok', 'gyeongdo', 'bu', 'mi', 'banmyeon', 'huk', 'byeonhyeongdo', 'jikgak', 'supyeong', 'jiryeok', 'su', 'tanseong', 'bunri', 'munje', 'bokjeom', 'ryeok', 'bokhap', 'song', 'gakdo', 'jeopchok', 'guseong', 'pyosibeop', 'hoejeon', 'gangpan', 'jeondal', 'jungang', 'yang', 'mul', 'hyeongtae', 'jikgyeong', 'byeon', 'hangbok', 'pyogi', 'gan', 'siteu', 'chukha', 'padan', 'wonri', 'gangui', 'iljeong', 'ingyeo', 'jang', 'jeon', 'byeonhyeongryul', 'wi', 'hoengdanmyeon', 'pyeonggyun', 'jikgyo', 'soseong', 'hando', 'jikseon', 'pandan', 'jeophap', 'gyeonghyang', 'sinroe', 'ryeok', 'banmyeon', 'si', 'an', 'jeolcha', 'jineung', 'gwajeong', 'pyeongga', 'paak', 'sangsu', 'gangui', 'yeolcheori', 'alruminyum', 'sik', 'byeon', 'iha', 'de', 'gwan', 'piro', 'gubun', 'beomwi', 'geurim', 'hwakin', 'jinpok', 'peulraenji', 'silje', 'hanjjok', 'teukseong', 'hanpyeon', 'siheomgi', 'pyojun', 'jucheol', 'gagong', 'gujo', 'jongryu', 'yeok', 'hwalyong', 'yeongyeol', 'unjeon', 'seramik', 'gochal', 'seontaek', 'gyeongu', 'nungeum', 'yeonsinyul', 'gyetong', 'sunasa', 'heoyong', 'jangryeok', 'chogwa', 'jisok', 'gang', 'jaeha', 'janryu', 'jungyo', 'gwanchal', 'munje', 'jaejil', 'yuri', 'jejo', 'gukbu', 'mokjeok', 'choecho', 'gangseong', 'tansoseong', 'susik', 'hyeonsang', 'geumsok', 'janggang', 'ryang', 'geukhan', 'baekbunyul', 'chukseon', 'yeongryul', 'dukke', 'tyubeu', 'yeonsin', 'gyeoljeong', 'pason', 'iron', 'botong', 'dodal', 'yeonghyang', 'jipjung', 'bokhap', 'pyo', 'goyu', 'amnasa', 'seuteurein', 'wonri', 'chunggyeok', 'jangryang', 'daebubun', 'uimi', 'pok', 'daeeung', 'hakseup', 'pyeongpan', 'apip', 'hukeu', 'byeonhyeongdo', 'injangsiheom', 'junggan', 'jogeon', 'won', 'jiji', 'yecheuk', 'geiji', 'jeondal', 'jungang', 'garo', 'dansun', 'chisu', 'chai', 'bujeongjeong', 'teukjeong', 'silheom', 'gangbong', 'cha', 'sanghwang', 'su', 'supyeong', 'yang', 'jiryeok', 'yuji', 'hyogwa', 'huk', 'gyeongdo', 'chukha', 'iljeong', 'yeje', 'gigye', 'mal', 'gyunyeol', 'bunri', 'ilchi', 'wijjok', 'mi', 'ingyeo', 'puasong', 'gangpan', 'eum', 'arae', 'hoengdanmyeon', 'yangjjok', 'bokjeom', 'sinjang', 'jikgak', 'bi', 'gan', 'bu', 'jeongryang', 'naenggan', 'ganggwan', 'eungdap', 'poham', 'pilyo', 'jeohang', 'gaji', 'seon', 'jeopchok', 'song', 'gap', 'gijun', 'guseong', 'chogi', 'pyosi', 'neungryeok', 'gojeong', 'hoejeon', 'ringkeu', 'jikgyeong', 'in', 'gakdo', 'pyosibeop', 'gicho', 'iyong', 'mul', 'jakeop', 'sijak', 'ap', 'gugan', 'hapryeok', 'jang', 'jijeom', 'suchuk', 'jeongyeokhak', 'jikgyo', 'siteu']]\n",
      "probability =  [[2.0256410256410255, 1.4273504273504272, 1.6615384615384614, 1.5598290598290596, 1.8034188034188032, 2.3311965811965814, 1.978021978021978, 1.4273504273504272, 1.4829059829059827, 1.566239316239316, 2.312354312354312, 1.316239316239316, 2.0923076923076924, 1.316239316239316, 0.5384615384615384, 1.6153846153846154, 1.5264957264957264, 1.6178266178266179, 1.5811965811965811, 0.9965811965811965, 1.7264957264957264, 1.4829059829059827, 1.393162393162393, 1.5940170940170941, 1.978021978021978, 1.8034188034188032, 2.413675213675214, 1.5957264957264956, 1.6495726495726495, 1.5598290598290596, 2.141310541310541, 1.4273504273504272, 2.0256410256410255, 2.547008547008547, 1.4985754985754984, 2.141310541310541, 1.566239316239316, 1.7264957264957264, 0.5384615384615384, 1.9145299145299144, 1.5042735042735043, 2.547008547008547, 1.601953601953602, 0.46153846153846156, 1.3345543345543347, 1.4893162393162394, 1.3345543345543347, 1.7264957264957264, 1.566239316239316, 2.1843711843711846, 1.566239316239316, 2.604884004884005, 1.9010989010989012, 1.6178266178266179, 1.0883190883190883, 1.4495726495726495, 1.7179487179487178, 0.46153846153846156, 1.4393162393162395, 1.770940170940171, 1.7264957264957264, 1.7264957264957264, 1.4114774114774113, 1.3623931623931624, 1.4114774114774113, 1.4615384615384617, 1.7264957264957264, 2.25982905982906, 1.5042735042735043, 2.2612942612942613, 2.1367521367521367, 0.5384615384615384, 1.6153846153846154, 1.152136752136752, 2.0256410256410255, 2.4789743589743587, 1.7264957264957264, 1.9487179487179487, 1.517094017094017, 0.5384615384615384, 2.0042735042735043, 0.5384615384615384, 0.5384615384615384, 1.4273504273504272, 1.601953601953602, 1.3345543345543347, 1.8632478632478633, 1.7264957264957264, 1.7264957264957264, 1.7264957264957264, 1.8376068376068375, 1.9145299145299144, 1.4985754985754984, 1.4273504273504272, 0.6153846153846154, 1.7931623931623932, 1.6178266178266179, 1.5264957264957264, 0.5384615384615384, 1.770940170940171, 1.2606837606837606, 1.4273504273504272, 2.3442307692307693, 2.712820512820513, 1.847863247863248, 2.0598290598290596, 0.5384615384615384, 1.978021978021978, 0.46153846153846156, 1.4495726495726495, 1.5598290598290596, 1.5042735042735043, 1.6153846153846154, 1.4068376068376067, 1.916239316239316, 0.5384615384615384, 1.9010989010989012, 1.4615384615384617, 1.4829059829059827, 1.5145299145299145, 1.9145299145299144, 0.5384615384615384, 1.4273504273504272, 0.5384615384615384, 1.2844932844932844, 0.5384615384615384, 1.7393162393162394, 0.5384615384615384, 1.3205128205128207, 1.9010989010989012, 1.678876678876679, 1.847863247863248, 1.3226495726495726, 0.5384615384615384, 2.2612942612942613, 1.6947496947496947, 1.9010989010989012, 0.5384615384615384, 1.3345543345543347, 1.566239316239316, 1.4495726495726495, 1.5264957264957264, 0.5384615384615384, 1.4273504273504272, 1.5264957264957264, 1.5042735042735043, 1.3623931623931624, 0.5384615384615384, 1.9010989010989012, 1.5264957264957264, 1.8653846153846154, 2.0946275946275943, 0.5384615384615384, 1.770940170940171, 1.770940170940171, 1.4615384615384617, 1.5760683760683758, 1.9010989010989012, 1.5940170940170941, 1.5598290598290596, 1.7264957264957264, 1.9487179487179487, 1.893162393162393, 1.9010989010989012, 1.6153846153846154, 1.4495726495726495, 2.170940170940171, 1.6923076923076923, 1.3903133903133902, 1.9010989010989012, 1.5264957264957264, 1.4495726495726495, 1.5264957264957264, 1.4893162393162394, 1.847863247863248, 1.4273504273504272, 1.3345543345543347, 1.9145299145299144, 0.5384615384615384, 1.9145299145299144, 1.678876678876679, 0.5384615384615384, 2.362881562881563, 1.1903651903651904, 0.6153846153846154, 1.3846153846153846, 1.7884615384615383, 0.6153846153846154, 1.4216524216524216, 1.770940170940171, 1.678876678876679, 2.3663817663817666, 2.1367521367521367, 1.4273504273504272, 2.3663817663817666, 1.5598290598290596, 1.9407814407814408, 1.8653846153846154, 1.3133903133903133, 1.3623931623931624, 1.517094017094017, 2.547008547008547, 1.1903651903651904, 0.46153846153846156, 2.362881562881563, 1.7931623931623932, 1.566239316239316, 1.4114774114774113, 1.4985754985754984, 1.152136752136752, 1.4615384615384617, 0.5384615384615384, 0.9965811965811965, 1.4495726495726495, 1.6178266178266179, 1.2844932844932844, 1.6153846153846154, 1.978021978021978, 1.5264957264957264, 1.4829059829059827, 1.5264957264957264, 1.678876678876679, 1.7264957264957264, 1.5042735042735043, 1.6178266178266179, 0.5384615384615384, 1.847863247863248, 1.601953601953602, 1.6178266178266179, 2.25982905982906, 1.5598290598290596, 1.6178266178266179, 1.6495726495726495, 1.7264957264957264, 1.8034188034188032, 1.5957264957264956, 1.4114774114774113, 1.3345543345543347, 1.5598290598290596, 1.4495726495726495, 2.0256410256410255, 1.4495726495726495, 1.8376068376068375, 1.4985754985754984, 1.0883190883190883, 1.4829059829059827, 1.9145299145299144, 1.4615384615384617, 1.4273504273504272, 1.4893162393162394, 1.4273504273504272, 0.5384615384615384, 1.770940170940171, 1.3623931623931624, 1.3345543345543347, 1.4495726495726495, 0.5384615384615384, 1.7264957264957264, 1.978021978021978, 0.6153846153846154, 2.547008547008547, 1.7264957264957264, 2.0256410256410255, 1.2606837606837606, 2.604884004884005, 0.5384615384615384, 1.770940170940171, 2.0946275946275943, 1.5264957264957264, 1.7264957264957264, 1.7264957264957264, 1.5145299145299145, 1.7264957264957264, 2.1367521367521367, 1.566239316239316, 1.9010989010989012, 0.5384615384615384, 1.6615384615384614, 1.4273504273504272, 1.9487179487179487, 0.5384615384615384, 1.770940170940171, 1.4273504273504272, 2.0256410256410255, 1.5940170940170941, 1.7393162393162394, 0.6153846153846154, 1.7179487179487178, 1.978021978021978, 1.5811965811965811, 2.141310541310541, 1.316239316239316, 2.312354312354312, 1.9010989010989012, 1.770940170940171, 1.4829059829059827, 1.770940170940171, 1.5598290598290596, 1.5264957264957264, 1.678876678876679, 1.3226495726495726, 0.5384615384615384, 1.4273504273504272, 1.9145299145299144, 1.2844932844932844, 1.847863247863248, 1.7264957264957264, 1.6153846153846154, 1.9010989010989012, 0.5384615384615384, 2.3442307692307693, 0.46153846153846156, 1.4273504273504272, 0.5384615384615384, 1.893162393162393, 0.5384615384615384, 1.5598290598290596, 1.566239316239316, 0.5384615384615384, 1.5042735042735043, 1.7264957264957264, 1.9487179487179487, 0.5384615384615384, 1.4393162393162395, 1.6153846153846154, 1.2844932844932844, 1.4615384615384617, 0.5384615384615384, 2.170940170940171, 1.7884615384615383, 0.46153846153846156, 1.9487179487179487, 2.1843711843711846, 1.3345543345543347, 2.3311965811965814, 1.4893162393162394, 2.1367521367521367, 0.5384615384615384, 0.5384615384615384, 1.566239316239316, 0.5384615384615384, 1.8632478632478633, 1.5042735042735043, 2.0598290598290596, 0.5384615384615384, 0.5384615384615384, 1.910683760683761, 1.5264957264957264, 1.5264957264957264, 1.3345543345543347, 2.0923076923076924, 1.7264957264957264, 1.9145299145299144, 1.4495726495726495, 1.5940170940170941, 1.9010989010989012, 1.4216524216524216, 1.678876678876679, 1.9010989010989012, 2.2612942612942613, 1.9010989010989012, 1.9145299145299144, 0.5384615384615384, 2.284900284900285, 1.3903133903133902, 2.413675213675214, 1.3345543345543347, 1.770940170940171, 1.6923076923076923, 1.4495726495726495, 1.4273504273504272, 1.5145299145299145, 1.847863247863248, 1.4273504273504272, 1.9010989010989012, 1.4273504273504272, 1.5598290598290596, 2.269230769230769, 1.6752136752136755, 0.5384615384615384, 1.5760683760683758, 2.170940170940171, 0.5384615384615384, 1.678876678876679, 1.4376068376068378, 1.4114774114774113, 2.0256410256410255, 1.978021978021978, 2.312354312354312, 1.316239316239316, 2.0923076923076924, 1.566239316239316, 0.5384615384615384, 1.8034188034188032, 1.9010989010989012, 1.601953601953602, 1.6495726495726495, 2.3311965811965814, 1.5598290598290596, 1.4273504273504272, 1.4273504273504272, 1.6615384615384614, 0.9965811965811965, 0.5384615384615384, 2.413675213675214, 1.6153846153846154, 1.7264957264957264, 1.5811965811965811, 1.4829059829059827, 1.5264957264957264, 1.8034188034188032, 1.5042735042735043, 1.393162393162393, 1.316239316239316, 1.4393162393162395, 0.5384615384615384, 1.3345543345543347, 1.4985754985754984, 1.566239316239316, 1.5940170940170941, 2.0256410256410255, 0.46153846153846156, 1.7264957264957264, 1.4829059829059827, 1.6178266178266179, 2.3442307692307693, 1.4893162393162394, 2.141310541310541, 2.0042735042735043, 1.6178266178266179, 1.7264957264957264, 2.1367521367521367, 2.2612942612942613, 1.3345543345543347, 2.25982905982906, 2.1843711843711846, 1.7264957264957264, 1.978021978021978, 1.0883190883190883, 1.7264957264957264, 1.4495726495726495, 1.5598290598290596, 1.770940170940171, 1.4273504273504272, 1.4273504273504272, 1.2606837606837606, 1.5264957264957264, 1.4273504273504272, 1.5598290598290596, 1.8376068376068375, 0.5384615384615384, 1.3345543345543347, 1.2844932844932844, 1.4615384615384617, 1.5957264957264956, 0.5384615384615384, 2.547008547008547, 1.4114774114774113, 1.566239316239316, 1.7931623931623932, 1.978021978021978, 1.3623931623931624, 1.3226495726495726, 2.141310541310541, 0.5384615384615384, 1.4273504273504272, 2.604884004884005, 1.916239316239316, 1.770940170940171, 1.9145299145299144, 1.7264957264957264, 1.847863247863248, 2.4789743589743587, 1.4114774114774113, 1.7264957264957264, 1.9487179487179487, 1.5042735042735043, 1.7264957264957264, 1.4068376068376067, 1.847863247863248, 1.152136752136752, 1.4615384615384617, 1.566239316239316, 0.5384615384615384, 1.6153846153846154, 1.6178266178266179, 0.46153846153846156, 2.547008547008547, 0.5384615384615384, 1.4495726495726495, 1.8632478632478633, 1.3205128205128207, 1.9010989010989012, 1.7264957264957264, 2.0256410256410255, 0.5384615384615384, 1.6947496947496947, 1.5264957264957264, 2.2612942612942613, 1.9010989010989012, 1.4829059829059827, 1.7264957264957264, 0.5384615384615384, 1.5598290598290596, 1.7179487179487178, 1.4985754985754984, 0.46153846153846156, 1.7393162393162394, 1.6153846153846154, 1.4615384615384617, 0.5384615384615384, 2.0598290598290596, 1.9145299145299144, 0.5384615384615384, 1.5598290598290596, 1.9487179487179487, 0.5384615384615384, 2.1367521367521367, 1.4273504273504272, 1.9010989010989012, 1.9145299145299144, 0.5384615384615384, 0.6153846153846154, 1.517094017094017, 1.5264957264957264, 0.5384615384615384, 1.4273504273504272, 0.5384615384615384, 1.4273504273504272, 1.601953601953602, 1.9010989010989012, 1.6153846153846154, 1.5042735042735043, 1.5760683760683758, 1.4893162393162394, 1.3345543345543347, 1.9010989010989012, 1.5042735042735043, 0.5384615384615384, 1.6923076923076923, 0.5384615384615384, 2.170940170940171, 1.893162393162393, 1.566239316239316, 1.9010989010989012, 1.847863247863248, 1.4495726495726495, 2.0946275946275943, 0.5384615384615384, 2.269230769230769, 1.5940170940170941, 1.9145299145299144, 0.6153846153846154, 1.5264957264957264, 1.9010989010989012, 1.5264957264957264, 1.770940170940171, 1.3623931623931624, 1.678876678876679, 1.678876678876679, 1.3345543345543347, 1.3903133903133902, 1.7264957264957264, 1.4495726495726495, 1.4216524216524216, 1.4495726495726495, 2.0923076923076924, 0.5384615384615384, 1.6495726495726495, 1.5264957264957264, 0.6153846153846154, 1.5145299145299145, 1.3226495726495726, 1.5598290598290596, 1.6495726495726495, 0.5384615384615384, 1.3846153846153846, 0.5384615384615384, 1.4495726495726495]]\n",
      "change_word =  ['요건', '이제', '그', '0강', '연강', '재료', '연강', '우리가', '주변에서', '흔히', '볼', '수', '있는']\n",
      "change_word_again =  ['요건', '이제', '그', '0강', '연강', '재료', '연강', '우리가', '주변에서', '흔히', '볼', '수', '있는']\n",
      "line_li =  ['요', '건', ' ', '이', '제', ' ', '그', ' ', '0', '강', ' ', '연', '안', ' ', '재', '료', ' ', '연', '강', ' ', '우', '리', '가', ' ', '주', '변', '에', '서', ' ', '흔', '히', ' ', '볼', ' ', '수', ' ', '있', '는']\n",
      "fi_line =  ['요', '건', '이', '제', '그', '0', '강', '연', '강', '재', '료', '연', '강', '우', '리', '가', '주', '변', '에', '서', '흔', '히', '볼', '수', '있', '는']\n",
      "요건이 제 그 0 강연강 재료연강우리가 주변에서 흔히 볼 수 있는\n"
     ]
    }
   ],
   "source": [
    "line=\"요건 이제 그 0강 연안 재료 연강 우리가 주변에서 흔히 볼 수 있는\"\n",
    "print(line)\n",
    "line_li=list(line)\n",
    "print(line_li)\n",
    "for i in range(len(line_li)-1):\n",
    "    if line_li[i] == '제':\n",
    "        if line_li[i+1] == ' ' and line_li[i-1] == ' ':\n",
    "            line_li[i] = '이제'\n",
    "\n",
    "for i in range(len(line_li)-1):\n",
    "    if line_li[i] == '자':\n",
    "        if line_li[i+1] == ' ' and line_li[i-1] == ' ':\n",
    "            line_li[i] = ' '\n",
    "\n",
    "\n",
    "line=''.join(line_li)\n",
    "\n",
    "jamak_nn = tagger.nouns(line) #자막에 나오는 의미있는 명사를 찾기\n",
    "jamak_morphs = tagger.morphs(line)\n",
    "line_for_space = line.split(' ')\n",
    "line_space = line_for_space.copy()\n",
    "print('line_space = ',line_space)\n",
    "for i in range(len(line_for_space)):\n",
    "    if 2 * i < len(line_for_space):\n",
    "        line_for_space.insert(2 * i + 1, 'space')\n",
    "\n",
    "for i in range(len(line_for_space)):\n",
    "    line_for_space[i] = tagger.morphs(line_for_space[i])\n",
    "\n",
    "line_space_include = sum(line_for_space,[])\n",
    "jamak_nn = tagger.nouns(line)\n",
    "space_jamak = ''.join(line_space_include)\n",
    "\n",
    "jamak_nouns=[]\n",
    "for w in jamak_nn:\n",
    "    if w not in except_words:\n",
    "        jamak_nouns.append(w)\n",
    "print(jamak_nouns)\n",
    "if tagger.morphs(space_jamak) != []: #자막문장을 품사별로 끊기\n",
    "    jamak = tagger.morphs(space_jamak)\n",
    "\n",
    "#print('jamak = ',jamak)\n",
    "\n",
    "error_word = find_error_word()\n",
    "print('error_word = ',error_word)\n",
    "err_word = comb_error_word()\n",
    "print('err_word = ',err_word)\n",
    "check_nouns = nearby_error_word()\n",
    "print('check_nouns = ',check_nouns)\n",
    "word_list = check_word_list()\n",
    "print('word_list = ', word_list)\n",
    "pronounce = romanizing()\n",
    "print('pronounce = ',pronounce)\n",
    "probability = similarity()\n",
    "print('probability = ',probability)\n",
    "change_word = word_change()\n",
    "print('change_word = ',change_word)\n",
    "change_word_again = word_change_again()\n",
    "print('change_word_again = ',change_word_again)\n",
    "\n",
    "# 자막으로 전환\n",
    "\n",
    "final_line = ''.join(change_word_again)\n",
    "errnum_convlist=['개','명','시','분','초','원','달','불','마','수','승','센','미','키','단','번','째']\n",
    "\n",
    "fi_li=tagger.morphs(final_line)\n",
    "\n",
    "for i in range(len(fi_li)-1):\n",
    "    if fi_li[i] == '4':\n",
    "        if fi_li[i+1] not in errnum_convlist:\n",
    "            fi_li[i] = ' 네 '\n",
    "    elif fi_li[len(fi_li)-1] == '4':\n",
    "        fi_li[(len(fi_li)-1)] = ' 네 '\n",
    "        \n",
    "for i in range(len(fi_li)-1):\n",
    "    if fi_li[i] == '5':\n",
    "        if fi_li[i+1] not in errnum_convlist:\n",
    "            fi_li[i] = ' 오 '\n",
    "    elif fi_li[len(fi_li)-1] == '5':\n",
    "        fi_li[(len(fi_li)-1)] = ' 오 '\n",
    "\n",
    "fi_line=''.join(fi_li)\n",
    "fi_line=list(fi_line)\n",
    "\n",
    "for i in range(len(fi_line)-1):\n",
    "    if fi_line[i] == '동':\n",
    "        if fi_line[i+1] == '기':\n",
    "            if fi_line[i-1] == ' ':\n",
    "                pass\n",
    "            else:\n",
    "                fi_line.insert(i,' ')\n",
    "\n",
    "for i in range(len(line)-1):\n",
    "    if line_li[i] == '예':\n",
    "        if line_li[i+1] == ' ' and line_li[i-1] == ' ':\n",
    "            for j in range(len(fi_line)-1):\n",
    "                if fi_line[j] == '예':\n",
    "                    fi_line.insert(j+1, ' ')\n",
    "\n",
    "print('line_li = ',line_li)\n",
    "print('fi_line = ',fi_line)\n",
    "fi_line=''.join(fi_line)\n",
    "\n",
    "result = spacing(fi_line)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
