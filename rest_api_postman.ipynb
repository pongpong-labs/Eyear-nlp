{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from eunjeon import Mecab\n",
    "from korean_romanizer import *\n",
    "import jellyfish\n",
    "from pykospacing import spacing\n",
    "\n",
    "app=Flask(__name__)\n",
    "\n",
    "model = Word2Vec.load('psychology.model')\n",
    "\n",
    "tagger = Mecab()\n",
    "\n",
    "text_list=[]\n",
    "with open('analysis_token.txt', 'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        text_list.append(line.rstrip())\n",
    "    \n",
    "except_words=[]\n",
    "with open('except_word.txt', 'r',encoding='utf-8') as f:\n",
    "    except_words = []\n",
    "    for line in f:\n",
    "        except_words.append(line.rstrip())\n",
    "    \n",
    "# MAIN\n",
    "def find_error_word(): #오류가 있는지 확인\n",
    "    global error_word\n",
    "    error_word=[]\n",
    "    for i in jamak_nouns:\n",
    "        if i not in text_list:\n",
    "            error_word.append(i)\n",
    "    return error_word\n",
    "\n",
    "def nearby_error_word(): #오류 단어 앞뒤의 3단어 뽑기\n",
    "    global check_nouns\n",
    "    check_nouns=[]#오류단어 앞뒤의 단어 저장\n",
    "    if error_word==[]:\n",
    "        return []\n",
    "    else:\n",
    "        for i in range(len(error_word)):\n",
    "            for j in range(len(jamak_nouns)):\n",
    "                if error_word[i] == jamak_nouns[j]:\n",
    "                    check_nouns_list=[]\n",
    "                    if j == len(jamak_nouns)-1:\n",
    "                        try:\n",
    "                            check_nouns_list.append(jamak_nouns[j-1])\n",
    "                            check_nouns_list.append(jamak_nouns[j-2])\n",
    "                            check_nouns_list.append(jamak_nouns[j-3])\n",
    "                            check_nouns_list.append(jamak_nouns[j-4])\n",
    "                        except:\n",
    "                            pass\n",
    "                    elif j == len(jamak_nouns)-2:\n",
    "                        try:\n",
    "                            check_nouns_list.append(jamak_nouns[j-1])\n",
    "                            check_nouns_list.append(jamak_nouns[j+1])\n",
    "                            check_nouns_list.append(jamak_nouns[j-2])\n",
    "                            check_nouns_list.append(jamak_nouns[j-3])\n",
    "                        except:\n",
    "                            pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            check_nouns_list.append(jamak_nouns[j-1])\n",
    "                            check_nouns_list.append(jamak_nouns[j+1])\n",
    "                            check_nouns_list.append(jamak_nouns[j-2])\n",
    "                            check_nouns_list.append(jamak_nouns[j+2])\n",
    "                        except:\n",
    "                            pass\n",
    "                    check_nouns.append(check_nouns_list)    \n",
    "    return check_nouns\n",
    "\n",
    "def check_word_list(): #오류 단어 근처의 단어에 대해 연관성 높은 단어의 리스트를 저장\n",
    "    global word_list\n",
    "    global model_result\n",
    "    word_list=[]\n",
    "    \n",
    "    if check_nouns==[]:\n",
    "        return []\n",
    "\n",
    "    else:   \n",
    "        for i in range(len(check_nouns)):\n",
    "            list_result=[]\n",
    "            for j in check_nouns[i]:\n",
    "                try:\n",
    "                    model_result=model.wv.most_similar(j,topn=50)\n",
    "                    for k in model_result:\n",
    "                        if k[0] not in except_words:\n",
    "                            list_result.append(k[0])\n",
    "                except:\n",
    "                    pass\n",
    "            word_list.append(list_result)\n",
    "\n",
    "    return word_list\n",
    "\n",
    "\n",
    "\n",
    "def romanizing(): #word_list의 한글 발음을 로마자로 변환\n",
    "    global pronounce\n",
    "    pronounce=[] # word_list의 발음을 저장.\n",
    "    if word_list == []:\n",
    "        return []\n",
    "    else:\n",
    "        for i in range(len(word_list)):\n",
    "            pronounce_list=[]\n",
    "            for j in range(len(word_list[i])):\n",
    "                try:\n",
    "                    a=Romanizer(word_list[i][j])\n",
    "                    pronounce_list.append(a.romanize())\n",
    "                except:\n",
    "                    pass\n",
    "            pronounce.append(pronounce_list)\n",
    "    return pronounce\n",
    "\n",
    "\n",
    "\n",
    "def similarity(): #유사도 측정\n",
    "    global probability\n",
    "    probability=[]\n",
    "    if pronounce == []:\n",
    "        return []\n",
    "    else:\n",
    "        for e in range(len(error_word)):\n",
    "            a=Romanizer(error_word[e]).romanize()\n",
    "            prob=[]\n",
    "            prob1=[]\n",
    "            prob2=[]\n",
    "            prob3=[]\n",
    "            for j in range(len(pronounce[e])):\n",
    "                prob1.append(jellyfish.jaro_winkler_similarity(a, pronounce[e][j]))\n",
    "                prob2.append(jellyfish.jaro_similarity(a, pronounce[e][j]))\n",
    "                prob3.append(1-(jellyfish.levenshtein_distance(a,pronounce[e][j]))/21)\n",
    "                prob.append(prob3[j]+(prob1[j]+prob2[j])/2)\n",
    "            probability.append(prob)\n",
    "    return probability\n",
    "\n",
    "\n",
    "\n",
    "def word_change(): #오류단어를 교체\n",
    "    global correct_word\n",
    "    correct_word=[]\n",
    "    if probability==[]:\n",
    "        return jamak\n",
    "    else:\n",
    "        change_word=[]\n",
    "        for i in range(len(probability)):\n",
    "            err_word_index=probability[i].index(max(probability[i]))\n",
    "            correct_word.append(word_list[i][err_word_index])\n",
    "\n",
    "        for a in range(len(jamak)):\n",
    "            for b in range(len(error_word)):\n",
    "                if jamak[a] == error_word[b]:\n",
    "                    jamak[a]=correct_word[b]\n",
    "    return jamak\n",
    "\n",
    "\n",
    "\n",
    "def process(line):\n",
    "    global jamak_nouns\n",
    "    global jamak\n",
    "    jamak_nn=tagger.nouns(line) #자막에 나오는 의미있는 명사를 찾기\n",
    "\n",
    "    jamak_nouns=[]\n",
    "    for w in jamak_nn:\n",
    "        if w not in except_words:\n",
    "            jamak_nouns.append(w)\n",
    "\n",
    "    if tagger.morphs(line)!=[]: #자막문장을 품사별로 끊기\n",
    "        jamak=tagger.morphs(line)\n",
    "\n",
    "    error_word=find_error_word()\n",
    "    check_nouns=nearby_error_word()\n",
    "    word_list=check_word_list()\n",
    "    pronounce=romanizing()\n",
    "    probability=similarity()\n",
    "    change_word=word_change()\n",
    "\n",
    "    # 자막으로 전환\n",
    "\n",
    "    jamak=''.join(jamak)\n",
    "\n",
    "    result=spacing(jamak)\n",
    "\n",
    "    return result\n",
    "\n",
    "@app.route(\"/correctSubtitle\", methods=['POST'])\n",
    "\n",
    "def correctSubtitle():\n",
    "    params = request.get_json()\n",
    "    result=process(params['subtitle'])\n",
    "    return {'result':result}\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='127.0.0.1',port=8000,debug=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
